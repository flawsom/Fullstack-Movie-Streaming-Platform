{"cells":[{"cell_type":"markdown","metadata":{},"source":["!pip install -q tensorflow-recommenders<br>\n","!pip install -q --upgrade tensorflow-datasets<br>\n","!pip install -q scann<br>\n","pip install tensorflow-io<br>\n","pip install kafka-python"]},{"cell_type":"markdown","metadata":{},"source":["Tensorflow"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-07-24 04:10:22.714429: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2022-07-24 04:10:22.714479: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"]},{"ename":"AttributeError","evalue":"module 'tensorflow.compat.v2.tpu.experimental' has no attribute 'HardwareFeature'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_247/4248168382.py\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjson\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_recommenders\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_io\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_recommenders/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_recommenders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_recommenders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;31m# Internal extension library import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_recommenders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_recommenders/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Experimental APIs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_recommenders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_recommenders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_recommenders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_recommenders/experimental/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Experimental layers APIs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_recommenders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_recommenders/experimental/layers/embedding/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Experimental embedding layers.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_recommenders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_tpu_embedding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPartialTPUEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_recommenders/experimental/layers/embedding/partial_tpu_embedding.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_recommenders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu_embedding_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTPUEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRaggedTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_recommenders/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_recommenders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_recommenders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_recommenders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfactorized_top_k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_recommenders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_interaction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_recommenders/layers/embedding/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Embedding layers.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_recommenders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu_embedding_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTPUEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_recommenders/layers/embedding/tpu_embedding_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0m_DUMMY_NAME\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"tpu_embedding_helper_dummy\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0m_EMBEDDING_V2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHardwareFeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddingFeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0m_EMBEDDING_V1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHardwareFeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddingFeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0m_EMBEDDING_UNSUPPORTED\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHardwareFeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddingFeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSUPPORTED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.compat.v2.tpu.experimental' has no attribute 'HardwareFeature'"]}],"source":["import os\n","import pprint\n","import tempfile\n","from typing import Dict, Text\n","import numpy as np\n","from json import loads\n","import tensorflow as tf\n","import tensorflow_recommenders as tfrs\n","import tensorflow_datasets as tfds\n","import tensorflow_io as tfio \n","from kafka import KafkaProducer, KafkaConsumer as kc\n","from kafka.errors import KafkaError\n","# from sklearn import train_test_split\n","import pandas as pd "]},{"cell_type":"markdown","metadata":{},"source":["Data Retrieval from the User Activity Logs"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'kc' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_247/2321803230.py\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Consumer for Movies Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# When a new movie is added, the mvoie is added to a queue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m movie_consumer = kc(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mKAFKA_TOPIC_NAME_MOVIES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mbootstrap_servers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mKAFKA_CLIENT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'kc' is not defined"]}],"source":["KAFKA_GROUP_ID='user_recommendation'\n","KAFKA_TOPIC_NAME_MOVIES = \"movie_topic\"\n","KAFKA_TOPIC_NAME_USER = \"user_analytics\"\n","KAFKA_CLIENT = \"localhost:9092\"\n","KAFKA_PRODUCER = \"localhost:9093\"\n","KAFKA_PRODUCER_TOPIC = \"recommended_movies\"\n","# Consumer for Movies Dataset\n","# When a new movie is added, the mvoie is added to a queue\n","movie_consumer = kc(\n","    KAFKA_TOPIC_NAME_MOVIES,\n","    bootstrap_servers=[KAFKA_CLIENT],\n","    enable_auto_commit=True,\n","    group_id=KAFKA_GROUP_ID,\n","    auto_offset_reset='earliest',\n","    # auto_commit_interval_ms=5000,\n","    session_timeout_ms=6000,\n","    # Decode the message comming from the producer\n","    value_deserializer=lambda x: loads(x.decode('utf-8'))\n",")\n","# Consumer for User Analytics Service\n","user_rating_consumer = kc(\n","    KAFKA_TOPIC_NAME_USER,\n","    bootstrap_servers=[KAFKA_CLIENT],\n","    enable_auto_commit=True,\n","    group_id=KAFKA_GROUP_ID,\n","    auto_offset_reset='earliest',\n","    session_timeout_ms=6000,\n","    value_deserializer=lambda x: loads(x.decode('utf-8'))\n",")\n","\n","for msg in user_rating_consumer:\n","    kafka_message = msg.value\n","    print(kafka_message)\n","    # ratings = tfds.load(kafka_message, split=\"train\")\n","# Movies data.\n","# ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n","# Features of all the available movies.\n","# The movies dataset contains the movie id, movie title and data on what\n","# genres it belongs to.\n","for msg in movie_consumer:\n","    movies = msg.value"]},{"cell_type":"markdown","metadata":{},"source":["User Ratings<br>\n","Consume messages from User Activity<br>\n","The ratings dataset returns a dictionary of movie id, user id, the assigned<br>\n","rating, timestamp, movie information, and user information"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'movies' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_16605/1942479957.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkafka_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# To fit and evaluate the mdoel, we need to split it into a training and evaluation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mshuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100_000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshuffle_each_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'movies' is not defined"]}],"source":["movies = tf.data.Dataset.from_tensor_slices(dict(movies)).map(lambda x: { x[\"title\"]}).batch(4)\n","ratings = tf.data.Dataset.from_tensor_slices(dict(kafka_message)).batch(4)\n","# To fit and evaluate the mdoel, we need to split it into a training and evaluation set\n","tf.random.set_seed(42)\n","shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n","train = shuffled.take(80_000)\n","test = shuffled.skip(80_000).take(20_000)"]},{"cell_type":"markdown","metadata":{},"source":["We need a vocabulary that maps a raw feature value to an integer in a contiguous range: this<br>\n","allows us to look up the corresponding embeddings in our embedding tables"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'movies' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_16605/37314982.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmovie_titles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmovies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1_000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0muser_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1_000_000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"user_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0munique_movie_titles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie_titles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0munique_user_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'movies' is not defined"]}],"source":["movie_titles = movies.batch(1_000)\n","user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n","unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n","unique_user_ids = np.unique(np.concatenate(list(user_ids)))"]},{"cell_type":"markdown","metadata":{},"source":["unique_movie_titles[:10]<br>\n","Higher values will correspond to models that may be more accureate, but will<br>\n","also be slower to fit and more prone to overfitting"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["EMBEDDING_DIMENSION = 32"]},{"cell_type":"markdown","metadata":{},"source":["Query Tower<br>\n","Define the model itself<br>\n","Convert user ids to integers, and then convert those to use embeddings via an<br>\n","Embedding layer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["user_model = tf.keras.Sequential([\n","  tf.keras.layers.StringLookup(\n","      vocabulary=unique_user_ids, mask_token=None),\n","  # We add an additional embedding to account for unknown tokens.\n","  tf.keras.layers.Embedding(len(unique_user_ids) + 1, EMBEDDING_DIMENSION)\n","])\n","# The Candidate Tower\n","movie_model = tf.keras.Sequential([\n","  tf.keras.layers.StringLookup(\n","      vocabulary=unique_movie_titles, mask_token=None),\n","  tf.keras.layers.Embedding(len(unique_movie_titles) + 1, EMBEDDING_DIMENSION)\n","])\n","print(movie_model)\n","metrics = tfrs.metrics.FactorizedTopK(candidates=movies.batch(128).map(movie_model))\n","task = tfrs.tasks.Retrieval(metrics=metrics)\n","print(metrics)\n","print(task)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MovielensModel(tfrs.Model): \n","  def __init__(self, user_model, movie_model):\n","    super().__init__()\n","    self.movie_model: tf.keras.Model = movie_model\n","    self.user_model: tf.keras.Model = user_model\n","    self.task: tf.keras.layers.Layer = task"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n","    # We pick out the user features and pass them into the user model.\n","    user_embeddings = self.user_model(features[\"user_id\"])\n","    # And pick out the movie features and pass them into the movie model,\n","    # getting embeddings back.\n","    print(user_embeddings)\n","    positive_movie_embeddings = self.movie_model(features[\"title\"])\n","    # The task computes the loss and the metrics.\n","    return self.task(user_embeddings, positive_movie_embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class NoBaseClassMovieLensModel(tf.keras.Model):\n","  def __init__(self, user_model, movie_model):\n","    super().__init__()\n","    self.movie_model: tf.keras.Model = movie_model\n","    self.user_model: tf.keras.Model = user_model \n","    self.task: tf.keras.layers.Layer =  task\n","  def train_step(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n","    # Set up a gradient tape to record gradients\n","    with tf.GradientTape() as tape:\n","      user_embeddings = self.user_model(features[\"user_id\"])\n","      positive_movie_embeddings = self.movie_model(features[\"title\"])\n","      loss = self.task(user_embeddings, positive_movie_embeddings)\n","      regularisation_loss = sum(self.losses)\n","      total_loss = loss + regularisation_loss\n","    gradients = tape.gradient(total_loss, self.trainable_variables)\n","    self.optimizer.apply_gradient(zip(gradients, self.trainable_variables))\n","    metrics = {metric.name: metric.result() for metric in self.metrics} \n","    metrics[\"total_loss\"] = total_loss\n","    metrics[\"loss\"] = loss\n","    metrics[\"regularisation_loss\"] = regularisation_loss\n","    print(metrics)\n","    return metrics "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["  def test_step(self, features: Dict[Text, tf.Tensor]) -> tf.Tensor:\n","    # loss computation \n","    user_embeddings = self.user_model(features[\"user_id\"])\n","    positive_movie_embeddings = self.movie_model(features[\"title\"])\n","    loss = self.task(user_embeddings, positive_movie_embeddings)\n","\n","    # handle regularisaton losses as well \n","    regularisation_loss = sum(self.losses)\n","    total_loss = loss + regularisation_loss\n","    metrics = {metric.name: metric.result() for metric in self.metrics} \n","    metrics[\"total_loss\"] = total_loss\n","    metrics[\"loss\"] = loss\n","    metrics[\"regularisation_loss\"] = regularisation_loss\n","    return metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = MovielensModel(user_model, movie_model)\n","model.compile(optimizer = tf.keras.optimizers.Adagrad(learning_rate = 0.1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cached_train = train.shuffle(100_000).batch(8192).cache()\n","cached_test = test.batch(4096).cache()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.fit(cached_train, epochs=3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.evaluate(cached_test, return_dict = True)"]},{"cell_type":"markdown","metadata":{},"source":["Making Predictions <br>\n","Create a model that takes in raw query features and "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)"]},{"cell_type":"markdown","metadata":{},"source":["Recommends movies out of the entire movies dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["recommended_movies = index.index_from_dataset(tf.data.Dataset.zip(\n","    (movies.batch(100), movies.batch(100).map(model.movie_model))))"]},{"cell_type":"markdown","metadata":{},"source":["Export the query model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with tempfile.TemporaryDirectory() as tmp:\n","  path = os.path.join(tmp, \"model\")\n","  # Save the index.\n","  tf.saved_model.save(\n","      index,\n","      path,\n","      options=tf.saved_model.SaveOptions(namespace_whitelist=[\"Scann\"])\n","  )\n","  # Load it back; can also be done in TensorFlow Serving.\n","  loaded = tf.saved_model.load(path)\n","  # Pass a user id in, get top predicted movie titles back.\n","  scores, titles = loaded([\"42\"])\n","  print(f\"Recommendations: {titles[0][ : 3]}\")"]},{"cell_type":"markdown","metadata":{},"source":["Kafka Producer<br>\n","Store the train and send new messages to kafka"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def error_callback():\n","  raise Exception('Error while sending data to kafka!')"]},{"cell_type":"markdown","metadata":{},"source":["Write to a seperate messaging queue using 'LocalHost:9093'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def write_to_kafka(topic_name, items):\n","  count = 0\n","  producer = KafkaProducer(bootstrap_servers=[KAFKA_PRODUCER])\n","  for msg, key in items:\n","    producer \\\n","      .send(topic_name, key=key.encode('utf-8'), value=msg.encode('utf-8')) \\\n","      .add_errback(error_callback)\n","    count +=1\n","  producer.flush()\n","  print(f'Wrote {0} messages into topic: {1}'.format(count, topic_name))"]},{"cell_type":"markdown","metadata":{},"source":["Write to kafka<br>\n","Send over the entire dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["write_to_kafka(KAFKA_PRODUCER_TOPIC, recommended_movies)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":2}
