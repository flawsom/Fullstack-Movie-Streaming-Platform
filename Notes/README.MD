# Netflix Clone w/ WASM 

### The information here is purely 'notebook' information (raw notes, thinking process). THIS IS NOT THE FORMAL SYSTEM DESIGN. 
## Why I did this:
I was largely inspired by this architecture. I wanted to see if I could quickly grasp these technologies really quickly and build a functioning product. I was not paid by anyone, I was driven by my own curiosity to build large distrbuted systems like Netflix, Apple Media or Amazon Prime.


## DEPLOY TO DOCKER/ KUBERNETES -> AWS, VERCEL

## Architecture of this project 
[
![architecture_sample](https://user-images.githubusercontent.com/85416532/179338067-ba374ff4-2825-4bff-a4a7-a0ae83085366.png)
](url)

## Tech stack
### Client 
- NextJS/React/TypeScript
- Tailwind CSS
- CSS + HTML
- Stripe Payments
- Firebase
 ### Service 
- Apollo-Federation Router (Rust)
- Async GraphQL 
- Actix-Web
- Redis
- Docker / Kubernetes
- Async SQLx + PostgreSQL
- ScyllaDB/ CassandraDB 
- Apache Kafka, 
##### Subject to change!
- Apache Spark (pySpark/ (tried with Scala))
- Hadoop(HDFS) 

- AWS System (check below)
- Elastic Stack (Elasticsearch, Kabana)
- Apache Beam and BigQuery
- Influx DB
- BigQuery ML,

### General Logging Stack  
- Prometheus
- Open Telemetry
- Grafana
- Jaeger
#### Open Telemetry integration
- To execute this example you need a running Jaeger instance.
You can launch one using Docker:
`docker run -d -p6831:6831/udp -p6832:6832/udp -p16686:16686 jaegertracing/all-in-one:latest`
#### Tracing
You can look at the exported traces in your browser by visiting http://localhost:16686.
Spans will be also printed to the console in JSON format, as structured log records.
### Scylla / Cassandra Monitoring Stack (Scylla Cloud Monitoring Service) 
- Prometheus and Grafana 
- Alert Manager
# System Requirements
- Asset Service 
- Account Service 
- Asset Ingestion Service
- Kafka-Search Ingest Service 
- Search Service
- Subscription Service
- #### User Activity Tracker (WIP)
- #### Recommendation Service  (WIP)
- Home service 

## Challenges
### Designing a unique ID generator in distributed systems. 
The requirements for Unique ID are listed as follows:
- ID must be unique
- Filled with numerical values,
- Fit inside 64 bit
- Can be ordered by date 
- It must be able to generate 10,000 unique IDs per second 
Logs: 
- My initial design was to use unique Ids which are generated by each services. It seemed to work initially but I faced Hot partitioning when I applied the same method on ScyllaDB/ Cassandra Nodes. 
- Since Unique ID contain non-numerics, I was not able to sort it properly. It required me to add another column to use time as my Cluster Key.
- UUID is also way too long to be used for my Partitioning key.
- When using Cassandra Keep in the mind the column ordering will differ on how you want it. Cassandra automatically orders its column alphabetically. This is especially true if you are using structs, make sure to 'DESCRIBE table_name' first before designing your Object. It will save you a bunch of headaches
- TODO! => Inserting genre on MoviesBlock Table -  the genre id does not increment along with the genre_table, when i insert new object, it always show zero or null biut not the actual value

#### Twitter Snowflake
- After few researches, I found the Snowflake approach which ensures that the ID fits within 64 bit. 

## Things to do: 
- Kubernetes -> AWS EC2, Router52, ECS
### Apollo Gateway to Apollo Router: 
Source: https://www.apollographql.com/blog/announcement/backend/apollo-router-our-graphql-federation-runtime-in-rust/
- Migrated from Apollo Gateway to Apollo Router thanks to performance benefits: Low Latency, Higher throughput and Predictable 
- Server-side caching

##### Build the App's Supergraph: 
`rover supergraph compose --config supergraph.yaml > supergraph.graphql`
##### Start Apollo Router by: 
`./router --supergraph supergraph.graphql `
##### Start Apollo With Router Yaml Configuration 
`./router -c router.yaml --log info  --supergraph supergraph.graphql`


### Recommendation Service 
NOTE: I was not able to build a standalone Recommnedation service, I experienced more errors that any productivity
main source: https://haocai1992.github.io/data/science/2022/01/13/build-recommendation-system-using-scala-spark-and-hadoop.html

#### The Big Query issue
- I have just successfully integrate a BQML model and create Recommended content using implicit feedback .
However there is one major problem. I cant seem to find a way to stream the data from BQ to my external Database. Either I choose to use the READ_API from Big query and bulk inserting these to my database, which means that I could be duplicating some datasets along the way and this is not ideal.
- BIG QUERY is NOT CHEAP
The overall issue is this:
BEAM --> BQ --> BQML --> BQ --> ? --> Consumer Service --> Cassandra

The one solution I could think of is instead of using BQML as my Machine learning tool. I could use 
TensorFlow to predict the movies for the users, In this way, Id be making a real time Recommendation System at a limited Cost as well (since BQ is prettty expensive)



### System Design 
## 1. User Service: Functional Requirements
- Users should be able to create, update and delete their account 
- Users should be able to manage multiple profiles 
- Users should be able to enter a membership/ subscription plan
#### Non-functional Requirements
- Highly Consistent, 
- Scalable, 
- Low Latency 

## 2. Subscription Service: 
Handled externally by Stripe API

## 3. Transcoder Service: Functional Requirements
- Check the video format of uploaded videos 
- Transcode the video into different resolutions
- ~~Perform content filtering and tag the video if it contains PG items 
- Users can watch the content either by streaming or downloading the entire video 
- Users can watch the content on a wide range of devices 
#### Non-functional Requirements
- Automated, 
- Scalable 
- Highly available (less video buffering)
- Cost effective 
### AWS
![image](https://user-images.githubusercontent.com/85416532/176547472-77a588d0-f2bb-4b38-b913-053b96e837d6.png)
- When a user uploads a video or photo S3 bucket, 
- The S3 bucket publishes an event which trigger a lambda function
- When the Lambda function is invoked, it creates an IAM role for Media Convert and a Transcode Policy to convert the video from mp4 into HLS/Dash 
#### V2: VOD Convert Workflow
Here, due to the extra processing for using Rekognition, I decided to forego from using it in my stack. 
1. Media Info - Extracts technical metadata of the video 
2. Media Convert - Transcode the video to 720p, 1080p and 2160p.
3. ~~Runs AWS Rekognition Content Moderation - Scan for inappropriate segments -> which tags the video with 'Explicit' [WIP]
4. Collects Transcoded videos in another S3 bucket
5. Send Notifications after the video is done.
6. Host video files in local CDN centers

##### New content processing workflow 
![image](https://user-images.githubusercontent.com/85416532/176556116-ca4d1884-6d50-4c80-80dc-c98b8764a9cb.png)
source: https://github.com/aws-solutions/video-on-demand-on-aws
Workflow is as follows: 
1. User uploads file to S3 
2. Lambda notifies MediaConvert the process the file into 3 different resolutions
3. Once the workflow is finished, a notification is sent back to the user
4. The new videos are uploaded in a new S3 bucket, made available locally by AWS Cloudfront

#### Intial Design Plan
- Initial design was to have a intermediary server to submit files to S3, then after benchmarking, i found the network bandwidth consumption from the client to server to s3 excessive. So i decided to go for direct uploads from the client to s3 instead. 
- this method greatly reduced the network bandwidth i was initilially faced with -> actually  this method made the system much more scalable


## 4. Asset Ingestion Service: Functional Requirements
- Content Creators should be to upload videos + store video metadata 
- It must be able to feed the users timeline 
- Store Videos in different geographical locations 
#### Non-Functional Requirements
- Network/ Database Fault Tolerant
- Highly Available Data to support rapid queries
- Fast Upload service 
- Low Latency between user visibility and distribution of video 
- Cost Effective 
- Secure

## Apache Kafka 
#### View Messages inside of our Messaging Queue
`docker exec --interactive --tty kafka_broker kafka-console-consumer --bootstrap-server localhost:9092 --topic movie_topic --from-beginning`

## 5. Search Service: Functional Requirements 
TODO!:
- Problem: Keeping Elasticsearch in sync with Scylla Db 
- To ensure that the data is accurately defined in elasticsearch, in any instance of issues, I made sure that
elasticsearch can directly communicate with Asset service to ask for the required data which would be sent over a Messaging queue. Another way is to allow for dual writes on both databases.  

- Users should be able to search any movies (ie. Full text search).
Not just by title, but return documents that contain the words that are related or relevant to the keywords
- It must include fuzzy searching giving the users the nearest result
- It must be able to 'guess' what the user could be searching( autocomplete)
#### Non-Functional Requirements
- Scalable and efficient, it must be able to handle high load of reads and writes too 
- Fault tolerant, the loss of any one servers must not affect the availability of the service 

### ElasticSearch and ELK Stack 
Currently, It is running without any Certificates (tls, ssl etc).
- Source: https://www.elastic.co/downloads/kibana
- DOCKER FORMAT: https://www.elastic.co/guide/en/kibana/current/docker.html
1. Start an ElasticSearch container for development or testing:
 - `docker network create elastic` 
 - `docker pull docker.elastic.co/elasticsearch/elasticsearch:8.2.3` 
 - `docker run --name es-node01 --net elastic -p 9200:9200 -p 9300:9300 -t docker.elastic.co/elasticsearch/elasticsearch:8.2.3`\
2. Start Kibana and Connect it to your Elasticsearch Container
 - `docker pull docker.elastic.co/kibana/kibana:8.2.3`
 - `docker run --name kib-01 --net elastic -p 5601:5601 docker.elastic.co/kibana/kibana:8.2.3`

#### Example:
`-> Elasticsearch security features have been automatically configured!
-> Authentication is enabled and cluster connections are encrypted.

->  Password for the elastic user (reset with `bin/elasticsearch-reset-password -u elastic`):
  x=QT+p-uU8DIXkbH96G6

->  HTTP CA certificate SHA-256 fingerprint:
  57d33a86e167dfb86440723c4cdc0392248026a2cce34861a74d5a19d70211c2

->  Configure Kibana to use this cluster:
* Run Kibana and click the configuration link in the terminal when Kibana starts.
* Copy the following enrollment token and paste it into Kibana in your browser (valid for the next 30 minutes):
  eyJ2ZXIiOiI4LjIuMyIsImFkciI6WyIxNzIuMTkuMC4yOjkyMDAiXSwiZmdyIjoiNTdkMzNhODZlMTY3ZGZiODY0NDA3MjNjNGNkYzAzOTIyNDgwMjZhMmNjZTM0ODYxYTc0ZDVhMTlkNzAyMTFjMiIsImtleSI6IktfdWJrb0VCUnpyQnlrNGlrb1FSOmU1ZzI4c3VEU3A2d3Y2emk2WHlvakEifQ==`

3. Login to Kibana: http://localhost:5601/app/home#/
    Enrollment key: Check your docker
    Username : elastic
    password: 'Found Inside your Docker'
#### How to start using Elasticsearch:
1. Create Index
2. Define a mapping type (schema)
3. Define Fields


## 6. Recommendation Service 
Collaborative Filtering and Spark ALS Model
- This is the closest to what I wanted to build but I ended up replacing it with a 
mix of both implicit and explicit feedbacks
`https://haocai1992.github.io/data/science/2022/01/13/build-recommendation-system-using-scala-spark-and-hadoop.html#4-runnning-in-spark`
#### Hadoop Cluster 
After several difficulties in getting HDFS running in local machine, I found that it's easier to run a docker container on Third Party Services, Dbs Etc.

So install to run the HDFS cluster:
source: https://towardsdatascience.com/hdfs-simple-docker-installation-guide-for-data-science-workflow-b3ca764fc94b 

1. Go to SpackCluster file and then into Docker-Hadoop. Build the docker compose
2. Connect to `http://localhost:9870/`

### Deploy Spark on Kubernetes 
`https://www.cloudnatively.com/deploy-spark-on-kubernetes/`
##### Issues faced: 
Hadoop and Spark eventually fell out of my datapipeline, as a result of the deadline and the 
endless amount of version control issues with pyspark and the current kafka client im using, 
I decided to migrate to using apache beam to simplify the data ETL for my recommendations 

### INFLUX DB
Install Influx db 
`sudo apt-get update && sudo apt-get install influxdb`
Start Influx DB instance 
`sudo service influxdb start`

Create Database
`create database user_activity`

Run the server 
`http://localhost:8086`
